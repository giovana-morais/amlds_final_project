{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bdc932-6bc7-4adc-92a1-b3e0a1525948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import neural_tangents as nt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, jit, jacfwd, jacrev, lax, random, vmap\n",
    "from jax.example_libraries import optimizers\n",
    "from neural_tangents import stax\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a2e6c-0cf3-4219-a7a3-f769a577046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.colormaps.get_cmap('tab20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea63845-1c95-4e6b-82d6-fd7ca1df6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 dimensions\n",
    "# trying to find polynomials of degree k = 2\n",
    "# a nn, would require 100 samples to converge\n",
    "# a ntk, would require 10000 samples to converge\n",
    "D = 100 # dimension\n",
    "P = 550 # number of samples\n",
    "N = 500 # layer width\n",
    "\n",
    "def target_fn(beta, X):\n",
    "        return (X.T @ beta)**2/2.0\n",
    "\n",
    "X = random.normal(random.PRNGKey(0), (D,P))/ jnp.sqrt(D)\n",
    "Xt = random.normal(random.PRNGKey(1), (D,1000))/ jnp.sqrt(D)\n",
    "beta = random.normal(random.PRNGKey(2), (D,))\n",
    "\n",
    "y = target_fn(beta, X)\n",
    "yt = target_fn(beta, Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3ca4f-bd00-41a2-8930-777d116d39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)\n",
    "print(Xt.shape, yt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6f794-73db-470c-a858-157396b2a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = random.normal(random.PRNGKey(0), (N, D))\n",
    "a = random.normal(random.PRNGKey(0), (N, ))\n",
    "print(W.shape, a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b65c4-601c-4dd6-857d-d8b2ebef6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [a, W]\n",
    "alpha = 1 # scaling parameter, NOT weight norm scale\n",
    "eps = 0.02\n",
    "\n",
    "def NN_func2(params,X):\n",
    "    global alpha\n",
    "    global eps\n",
    "\n",
    "    a, W = params\n",
    "    D = W.shape[1]\n",
    "    N = a.shape[0]\n",
    "    h = W @ X.T\n",
    "\n",
    "    f = alpha * np.mean(phi(h,eps),axis=0) # w/o readouts\n",
    "    return f\n",
    "\n",
    "\n",
    "def phi(z, eps = 0.25):\n",
    "        return z + 0.5*eps*z**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa34c6-af93-4402-9aa1-579e77326b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk_fn = nt.empirical_ntk_fn(\n",
    "    NN_func2, vmap_axes=0, trace_axes=())\n",
    "\n",
    "def kernel_regression(X, y, Xt, yt, params, which='test'):\n",
    "    K_train = ntk_fn(X.T, None, params)\n",
    "    \n",
    "    a = jnp.linalg.solve(K_train, y)\n",
    "    \n",
    "    def estimate(xt):\n",
    "        k_test_train = ntk_fn(Xt.T, X.T, params)\n",
    "        k_test_train_squeezed = jnp.squeeze(k_test_train)\n",
    "        return jnp.dot(k_test_train_squeezed, a)\n",
    "    \n",
    "    estimates = vmap(estimate)(Xt.T if which=='test' else X.T)\n",
    "    labels = yt if which=='test' else y\n",
    "    mse = jnp.mean((estimates - labels) ** 2)\n",
    "    return mse, K_train\n",
    "\n",
    "\n",
    "def kalignment(K, train_y):\n",
    "    train_yc = train_y.reshape(-1, 1)\n",
    "    train_yc = train_yc - train_yc.mean(axis=0)\n",
    "    Kc = K - K.mean(axis=0)\n",
    "    top = jnp.dot(jnp.dot(train_yc.T, Kc), train_yc)\n",
    "    bottom = jnp.linalg.norm(Kc) * (jnp.linalg.norm(train_yc)**2)\n",
    "    return jnp.trace(top)/bottom\n",
    "\n",
    "\n",
    "def get_norms(K):\n",
    "    \"\"\"\n",
    "    given a kernel K, returns Frobenius norm, spectral norm and condition number\n",
    "    \"\"\"\n",
    "    eigenval, eigenvec = jax.numpy.linalg.eigh(K)\n",
    "    frobenius_norm = jnp.sqrt(np.sum(eigenval**2))\n",
    "    spectral_norm = max(eigenval)\n",
    "    condition_number = max(eigenval) / min(eigenval)\n",
    "\n",
    "    return frobenius_norm, spectral_norm, condition_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bcdcb-9abd-4534-9f07-5328bba19d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, K_train = kernel_regression(X, y, Xt, yt, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee37e12-f095-4c2c-9024-d4b06fa50ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving k_0 for reference\n",
    "K_0 = ntk_fn(Xt.T, None, params)\n",
    "np.save(f\"kernels/k_test_0\", K_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900c8b0-a2a0-4615-a51a-db28612edc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmse, _ = kernel_regression(X, y, Xt, yt, params)\n",
    "\n",
    "alphas = [1]\n",
    "epsilons = [0.02]\n",
    "epochs = 100000\n",
    "CENTER_LOSS = False\n",
    "TRAIN_READOUTS = False\n",
    "ntk_interval = 100\n",
    "COMPUTE_NORMS=True\n",
    "\n",
    "for alpha in alphas:\n",
    "    for eps in epsilons:\n",
    "        kaligns_test = []\n",
    "        epochs_to_plot = []\n",
    "        dots = []\n",
    "        \n",
    "        Cs, As = [], []\n",
    "        actual_w1aligns, actual_w2aligns = [], []\n",
    "        w1_aligns, w2_aligns = [], []\n",
    "        w1_vars, w2_vars, ws_covs = [], [], []\n",
    "        vars_compute_interval = 50\n",
    "        \n",
    "        lamb = 0\n",
    "        eta = N/alpha**2\n",
    "        opt_init, opt_update, get_params = optimizers.sgd(eta)\n",
    "        opt_init_lin, opt_update_lin, get_params_lin = optimizers.sgd(eta)\n",
    "        \n",
    "        opt_state = opt_init(params)\n",
    "        opt_state_lin = opt_init_lin(params)\n",
    "        \n",
    "        f_lin = nt.linearize(NN_func2, params)\n",
    "        lin_tr_losses = []\n",
    "        lin_te_losses = []\n",
    "        \n",
    "        if CENTER_LOSS:\n",
    "            loss_fn = jit(lambda p, X, y: jnp.mean( ( NN_func2(p, X.T)- NN_func2(params,X.T) - y )**2))\n",
    "        else:\n",
    "            loss_fn = jit(lambda p, X, y: jnp.mean( ( NN_func2(p, X.T) - y )**2 ))\n",
    "        \n",
    "        f_lin0 = nt.linearize(NN_func2, params)\n",
    "        lin_loss = jit(lambda p, X, y: jnp.mean((f_lin(p, X.T) - f_lin0(params, X.T) - y)**2)  )\n",
    "        grad_loss_lin = jit(grad(lin_loss, 0))\n",
    "        \n",
    "        reg_loss = jit(lambda p, X, y: loss_fn(p,X,y) + lamb / alpha * optimizers.l2_norm(p)**2 )\n",
    "        \n",
    "        grad_loss = jit(grad(reg_loss,0))\n",
    "        \n",
    "        tr_losses = []\n",
    "        te_losses = []\n",
    "\n",
    "        # measures of the kernel change\n",
    "        k_test_frob = []    \n",
    "        k_test_spec = []        \n",
    "        k_test_cond = []        \n",
    "        \n",
    "        alignments, alignmentst = [], []\n",
    "        epochs_to_plot = []\n",
    "        \n",
    "        t1s, t2s, t3s, epochs_to_compute = [], [], [], []\n",
    "        t1sm, t2sm, t3sm, ts_summ = [], [], [], []\n",
    "        ts_sum = []\n",
    "        alignments, alignmentst = [], []\n",
    "        \n",
    "        kmse, _ = kernel_regression(X, y, Xt, yt, get_params(opt_state))\n",
    "        \n",
    "        for t in tqdm(range(epochs)):\n",
    "            opt_state = opt_update(t, grad_loss(get_params(opt_state), X, y), opt_state)\n",
    "            pars = get_params(opt_state)\n",
    "            \n",
    "            train_loss = loss_fn(pars, X, y)\n",
    "            test_loss = loss_fn(pars, Xt, yt)\n",
    "            tr_losses += [train_loss]\n",
    "            te_losses += [test_loss]\n",
    "            \n",
    "            # new update rule for f_lin to compare learning curves\n",
    "            lin_pars = get_params_lin(opt_state_lin)\n",
    "            opt_state_lin = opt_update_lin(t, grad_loss_lin(lin_pars, X, y), opt_state_lin)\n",
    "            \n",
    "            lin_tr_losses += [ lin_loss(lin_pars, X, y) ]\n",
    "            lin_te_losses += [ lin_loss(lin_pars, Xt, yt) ]\n",
    "        \n",
    "            if t % vars_compute_interval == 0:\n",
    "                epochs_to_compute.append(t)\n",
    "            if t % ntk_interval == 0 and t > 0:\n",
    "                K_test = ntk_fn(Xt.T, None, pars)\n",
    "                #np.save(f\"kernels/k_test_{t}\", K_test)\n",
    "                cka_test = kalignment(K_test, yt)\n",
    "                kaligns_test += [ cka_test ]\n",
    "\n",
    "                frob_norm, spectral_norm, condition_number = get_norms(K_test)\n",
    "                k_test_frob += [frob_norm]\n",
    "                k_test_spec += [spectral_norm]\n",
    "                k_test_cond += [condition_number]\n",
    "                \n",
    "            \n",
    "            if t % 5000 == 0 and t > 0:\n",
    "                max_t = t\n",
    "                t_values = np.arange(0, max_t, ntk_interval)\n",
    "                interpolator = interp1d(t_values, kaligns_test, kind='linear', fill_value='extrapolate')\n",
    "                interpolated_kaligns = interpolator(np.arange(max_t))\n",
    "\n",
    "                # interpolate norms\n",
    "                interpolator_spec = interp1d(t_values, k_test_spec, kind='linear', fill_value='extrapolate')\n",
    "                interpolated_spec = interpolator_spec(np.arange(max_t))\n",
    "\n",
    "                interpolator_frob = interp1d(t_values, k_test_frob, kind='linear', fill_value='extrapolate')\n",
    "                interpolated_frob = interpolator_frob(np.arange(max_t))\n",
    "\n",
    "                interpolator_cond = interp1d(t_values, k_test_cond, kind='linear', fill_value='extrapolate')\n",
    "                interpolated_cond = interpolator_cond(np.arange(max_t))\n",
    "                \n",
    "                # save weights\n",
    "                np.save(f\"weights/w_{t}\", pars[1])\n",
    "                np.save(f\"weights/a_{t}\", pars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61a3b0-7f80-4b3a-8636-7a48e30dc9e0",
   "metadata": {},
   "source": [
    "# Kernel evolution with respect with $K_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50e716-ebe8-4400-a6de-431388be8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a7d4c-6a4e-4fe5-9178-fde89b73b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels_list = glob.glob(\"kernels/k_test_*.npy\")\n",
    "kernels_list.sort(key=natural_keys)\n",
    "\n",
    "def frobenius(x):\n",
    "    eigenval, eigenvec = jax.numpy.linalg.eigh(x)\n",
    "    frobenius_norm = jnp.sqrt(np.sum(eigenval**2))\n",
    "    return frobenius_norm\n",
    "\n",
    "k_0 = np.load(kernels_list[0])\n",
    "k_test = kernels_list\n",
    "indexes = [k.split(\"_\")[-1][:-4] for k in kernels_list]\n",
    "\n",
    "all_fb = []\n",
    "for k in k_test:\n",
    "    k_t = np.load(k)\n",
    "    fb = jnp.linalg.norm(k_t-k_0)\n",
    "    all_fb.append(fb)\n",
    "    #print(f\"k0 - {k.split('/')[-1]}\", fb)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(all_fb)\n",
    "ax.set_ylabel(r\"Frobenius norm $\\|K_t - K_0\\|_F$\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_xticks(range(0,len(indexes[::100])*100,100), indexes[::100], rotation=45)\n",
    "plt.savefig(\"figs/kernel_evolution.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ed098f-cc3e-4cce-bd72-0052ed59a4c5",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4e463-2f18-47d1-8fb0-ded84601d4df",
   "metadata": {},
   "source": [
    "## Train Losses and NTK alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24388486-d94e-4ce2-8051-28deb504893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "col = cmap(0)\n",
    "ax1.plot(np.array(tr_losses), linestyle='--', label=rf'Train Loss', color=col, lw=2)\n",
    "ax1.plot(np.array(te_losses), label=rf'Test Loss', color=col, lw=2)\n",
    "ax1.plot(np.array(lin_tr_losses), color='black', linestyle='--', label=f'Linearized train loss')\n",
    "ax1.plot(np.array(lin_te_losses), color='black', label=f'Linearized test loss')\n",
    "\n",
    "ax1.axhline(kmse, color='r', label=rf'$K_0$ regression MSE')\n",
    "ax1.set_xlabel('Epochs', fontsize=20)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_ylabel('MSE', fontsize=20)\n",
    "ax1.legend(loc='lower left', bbox_to_anchor=(0, 0.05))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(interpolated_kaligns, linestyle='--', color='green', label=r'NTK alignment, $\\frac{y^T K_0y}{||K_0||_F||y||^2}$', lw=2)\n",
    "ax2.legend(loc='upper left', bbox_to_anchor=(0, 0.5))\n",
    "ax2.set_ylabel('NTK alignment', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/main_metrics.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb2e23-d384-4835-b6d0-665ea53bb8fd",
   "metadata": {},
   "source": [
    "## MSE vs Spectral Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d6871-92b6-4d51-a574-451812fef3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "col = cmap(0)\n",
    "ax1.plot(np.array(tr_losses), linestyle='--', label=rf'Train Loss', color=col, lw=2)\n",
    "ax1.plot(np.array(te_losses), label=rf'Test Loss', color=col, lw=2)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(interpolated_spec, color='red', label=f'Spectral Norm')\n",
    "ax2.legend(loc='upper left', bbox_to_anchor=(0, 0.45))\n",
    "ax2.set_ylabel('Spectral Norm', fontsize=20)\n",
    "\n",
    "ax1.set_xlabel('Epochs', fontsize=20)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_ylabel('MSE', fontsize=20)\n",
    "ax1.legend(loc='lower left', bbox_to_anchor=(0, 0.20))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/loss_vs_spectral_norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe275a3c-2c15-417d-b38f-8eb631c2165a",
   "metadata": {},
   "source": [
    "## MSE vs Frobenius Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1df831-a466-4897-ae44-82d722657d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "col = cmap(0)\n",
    "ax1.plot(np.array(tr_losses), linestyle='--', label=rf'Train Loss', color=col, lw=2)\n",
    "ax1.plot(np.array(te_losses), label=rf'Test Loss', color=col, lw=2)\n",
    "\n",
    "\n",
    "ax1.set_xlabel('Epochs', fontsize=20)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_ylabel('MSE', fontsize=20)\n",
    "ax1.legend(loc='lower left', bbox_to_anchor=(0, 0.2))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(interpolated_frob, color='green', label=f'Frobenius Norm')\n",
    "ax2.legend(loc='upper left', bbox_to_anchor=(0, 0.58))\n",
    "ax2.set_ylabel('Frobenius Norm', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/loss_vs_frobenius.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37521d30-3e50-4bc9-8dd5-58147aec5f63",
   "metadata": {},
   "source": [
    "## MSE vs Condition Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cea356-bd98-4db3-ae2c-c88c6f9564f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "col = cmap(0)\n",
    "ax1.plot(np.array(tr_losses), linestyle='--', label=rf'Train Loss', color=col, lw=2)\n",
    "ax1.plot(np.array(te_losses), label=rf'Test Loss', color=col, lw=2)\n",
    "ax1.set_xlabel('Epochs', fontsize=20)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_ylabel('MSE', fontsize=20)\n",
    "ax1.legend(loc='lower left', bbox_to_anchor=(0, 0.1))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(interpolated_cond, linestyle='-', color='orange', label=r'Condition Number', lw=2)\n",
    "ax2.legend(loc='upper left', bbox_to_anchor=(0, 0.38))\n",
    "ax2.set_ylabel('Condition Number', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/loss_vs_condition.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc535c4-9548-4e82-b872-69a26fd6db01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
