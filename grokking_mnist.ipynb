{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c301076-d89c-42bd-8af9-d3698773560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe20047-20a1-463c-bb3b-72ebac666aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grokking on MNIST. Architectural details taken from Omnigrok paper: https://arxiv.org/pdf/2210.01117.pdf\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Download MNIST dataset\n",
    "mnist_dataset = MNIST(root=\"./data\", train=True, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fef22-28e7-44d8-a8ea-d66a85c8192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(params, x):\n",
    "  w0 = params[0]\n",
    "  h = w0 @ x.T / jnp.sqrt(w0.shape[1]) # N x P\n",
    "  for l, Wl in enumerate(params[1:-1]):\n",
    "    phi = h * (h > 0.0)\n",
    "    h = 1/jnp.sqrt(Wl.shape[1]) * Wl @ phi\n",
    "\n",
    "  phi = h * (h > 0.0)\n",
    "  f = phi.T @ params[-1].T / phi.shape[0]\n",
    "  return f\n",
    "\n",
    "def init_params(N, D, L, key, w_scale = 1.0):\n",
    "\n",
    "  params = [ w_scale * random.normal(key, (N,D)) ]\n",
    "  for l in range(L-1):\n",
    "    key, _ = random.split(key)\n",
    "    params += [ w_scale * random.normal(key,(N,N)) ]\n",
    "\n",
    "  params += [ w_scale * random.normal(key, (10,N)) ]\n",
    "  return params\n",
    "\n",
    "\n",
    "subset_size = 1000\n",
    "train_set, test_set = random_split(mnist_dataset, [subset_size, len(mnist_dataset) - subset_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=subset_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=2*subset_size)\n",
    "\n",
    "for X,y in train_loader:\n",
    "  break\n",
    "\n",
    "X = jnp.array( X.numpy() )\n",
    "y = jnp.array( y.numpy() )\n",
    "\n",
    "\n",
    "X = X.reshape((X.shape[0], X.shape[-2] * X.shape[-1]))\n",
    "\n",
    "for Xte, yte in test_loader:\n",
    "  break\n",
    "\n",
    "\n",
    "Xte = jnp.array(Xte.numpy())\n",
    "yte = jnp.array(yte.numpy())\n",
    "\n",
    "Xte= Xte.reshape((Xte.shape[0], Xte.shape[-2] * Xte.shape[-1]))\n",
    "\n",
    "\n",
    "y = jnp.eye(10)[y]\n",
    "yte = jnp.eye(10)[yte]\n",
    "\n",
    "\n",
    "scales = [1e-3, 0.01, 0.05, 0.5]\n",
    "traccs, taccs = [], []\n",
    "\n",
    "for scale in tqdm(scales):\n",
    "  # Constants and hyper-params\n",
    "  torch.manual_seed(42)\n",
    "  N = 200\n",
    "  D = 784\n",
    "  L = 3\n",
    "  weight_scale = 150.0  # Scaling factor for Kaiming initialization, replicated from the Omnigrok paper\n",
    "  lr = 1e-3\n",
    "  wd = 0.01\n",
    "  T = 250000\n",
    "  batch = 200\n",
    "  key = random.PRNGKey(0)\n",
    "\n",
    "  # Model\n",
    "  def MLP(params, x):\n",
    "      h = params[0] @ x.T / jnp.sqrt(params[0].shape[1])  # N x P\n",
    "      for l, Wl in enumerate(params[1:-1]):\n",
    "          h = h * (h > 0.0)\n",
    "          h = Wl @ h / jnp.sqrt(Wl.shape[1])\n",
    "      h = h * (h > 0.0)\n",
    "      f = h.T @ params[-1].T / h.shape[0]\n",
    "      return f*scale\n",
    "\n",
    "  key = random.PRNGKey(0)\n",
    "  params = init_params(N, D, L, key, weight_scale)\n",
    "\n",
    "  # Initialization with Kaiming scaling.\n",
    "  def init_params(N, D, L, key, weight_scale=1.0):\n",
    "      params = [weight_scale * random.normal(key, (N, D)) * jnp.sqrt(2. / D)]\n",
    "      for l in range(L-1):\n",
    "          key, _ = random.split(key)\n",
    "          params += [weight_scale * random.normal(key, (N, N)) * jnp.sqrt(2. / N)]\n",
    "      params += [weight_scale * random.normal(key, (10, N)) * jnp.sqrt(2. / N)]\n",
    "      return params\n",
    "\n",
    "\n",
    "  loss_fn = jit(lambda p, X, y: jnp.mean((MLP(p, X) - y)**2))\n",
    "  grad_fn = jit(grad(loss_fn))\n",
    "  optimizer = optax.adamw(learning_rate=lr, weight_decay=wd)\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  train_loss = []\n",
    "  test_loss = []\n",
    "  train_accuracy = []\n",
    "  test_accuracy = []\n",
    "\n",
    "  def compute_accuracy(predictions, targets):\n",
    "      return jnp.mean(jnp.argmax(predictions, axis=1) == jnp.argmax(targets, axis=1))\n",
    "\n",
    "\n",
    "  compute_every = 100\n",
    "  for t in range(T):\n",
    "      if t % compute_every == 0:\n",
    "          train_pred = MLP(params, X)\n",
    "          test_pred = MLP(params, Xte)\n",
    "\n",
    "          # Compute and store train & test loss\n",
    "          train_loss.append(loss_fn(params, X, y))\n",
    "          test_loss.append(loss_fn(params, Xte, yte))\n",
    "\n",
    "          # Compute and store train & test accuracy\n",
    "          train_accuracy.append(compute_accuracy(train_pred, y))\n",
    "          test_accuracy.append(compute_accuracy(test_pred, yte))\n",
    "\n",
    "      ind = batch * t % subset_size # take a slice of 200 out of our 1000 size dataset and cycle like that\n",
    "      grads = grad_fn(params, X[ind:ind+batch], y[ind:ind+batch])\n",
    "      updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "      params = optax.apply_updates(params, updates)\n",
    "\n",
    "  traccs += [ train_accuracy ]\n",
    "  taccs += [ test_accuracy ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbace4-b612-497e-b2b2-1d1e8de8dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'orange', 'purple', 'pink']\n",
    "for i, acc in enumerate(traccs):\n",
    "    x = np.arange(0, compute_every * len(acc), compute_every)\n",
    "    xnew = np.linspace(0, 250000, 250000)\n",
    "    spl = make_interp_spline(x, traccs[i], k=3) # k=3 for cubic spline\n",
    "    y_train_smooth = spl(xnew)\n",
    "    spl = make_interp_spline(x, taccs[i], k=3)\n",
    "    y_test_smooth = spl(xnew)\n",
    "\n",
    "    plt.plot(xnew, y_train_smooth*100, label=rf'Train Accuracy, $\\alpha$={scales[i]}', linestyle='--', color=colors[-i])\n",
    "    plt.plot(xnew, y_test_smooth*100, label=rf'Test Accuracy, $\\alpha$={scales[i]}', color=colors[-i])\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Accuracy', fontsize=20)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
