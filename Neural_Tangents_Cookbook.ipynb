{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIkq2O7yVWYJ"
   },
   "source": [
    "Copyright 2019 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "Lt74vgCVNN2b",
    "outputId": "19823683-4983-47ae-94e5-7f602d329dc2"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "from jax import random\n",
    "from jax.example_libraries import optimizers\n",
    "from jax import jit, grad, vmap\n",
    "\n",
    "import functools\n",
    "\n",
    "import neural_tangents as nt\n",
    "from neural_tangents import stax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8lzkWnROEQs"
   },
   "outputs": [],
   "source": [
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.3)\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".95\"})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def format_plot(x=None, y=None):\n",
    "  # plt.grid(False)\n",
    "  ax = plt.gca()\n",
    "  if x is not None:\n",
    "    plt.xlabel(x, fontsize=20)\n",
    "  if y is not None:\n",
    "    plt.ylabel(y, fontsize=20)\n",
    "\n",
    "def finalize_plot(shape=(1, 1)):\n",
    "  plt.gcf().set_size_inches(\n",
    "    shape[0] * 1.5 * plt.gcf().get_size_inches()[1],\n",
    "    shape[1] * 1.5 * plt.gcf().get_size_inches()[1])\n",
    "  plt.tight_layout()\n",
    "\n",
    "legend = functools.partial(plt.legend, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLa63IKQRnE0"
   },
   "outputs": [],
   "source": [
    "def plot_fn(train, test, *fs):\n",
    "  train_xs, train_ys = train\n",
    "\n",
    "  plt.plot(train_xs, train_ys, 'ro', markersize=10, label='train')\n",
    "\n",
    "  if test != None:\n",
    "    test_xs, test_ys = test\n",
    "    plt.plot(test_xs, test_ys, 'k--', linewidth=3, label='$f(x)$')\n",
    "\n",
    "    for f in fs:\n",
    "      plt.plot(test_xs, f(test_xs), '-', linewidth=3)\n",
    "\n",
    "  plt.xlim([-jnp.pi, jnp.pi])\n",
    "  plt.ylim([-1.5, 1.5])\n",
    "\n",
    "  format_plot('$x$', '$f$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zNUrSHJQOan"
   },
   "outputs": [],
   "source": [
    "def loss_fn(predict_fn, ys, t, xs=None):\n",
    "  mean, cov = predict_fn(t=t, get='ntk', x_test=xs, compute_cov=True)\n",
    "  mean = jnp.reshape(mean, mean.shape[:1] + (-1,))\n",
    "  var = jnp.diagonal(cov, axis1=1, axis2=2)\n",
    "  ys = jnp.reshape(ys, (1, -1))\n",
    "\n",
    "  mean_predictions = 0.5 * jnp.mean(ys ** 2 - 2 * mean * ys + var + mean ** 2,\n",
    "                                   axis=1)\n",
    "\n",
    "  return mean_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOVCymQgVn9R"
   },
   "source": [
    "# Neural Tangents Cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKnYe66NVzFF"
   },
   "source": [
    "In this notebook we explore the training of infinitely-wide, Bayesian, neural networks using a library called [Neural Tangents](https://github.com/google/neural-tangents). Recent work has shown that such networks are Gaussian Processes with a particular compositional kernel called the NNGP kernel. More recently, it was shown that predictions resulting from these networks following Gradient Descent are Gaussian with a distribution that can be computed in closed form using the [Neural Tangent Kernel](). Neural Tangents provides a high level library to compute NNGP and NT kernels for a wide range of neural networks. See [the paper]() for a more detailed description of the library itself.\n",
    "\n",
    "Our goal will be to train an ensemble of neural networks on a simple synthetic task. We'll then compare the results of this ensemble with the prediction of the NTK theory. Finally, we'll play around with different neural network architectures to see how this affects the resulting kernel. However, Neural Tangents is built on JAX which may be new to you. To get warmed up with JAX, we'll start out by generating some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM9rp-EqOuWT"
   },
   "source": [
    "## Warm Up: Creating a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEjKnmJsa6AC"
   },
   "source": [
    "We're going to build a widely used synthetic dataset that's used extensively in [Pattern Recognition and Machine Learning](). Incidentally, Pattern Recognition and Machine Learning is an outstanding book by Christopher Bishop that was recently released for free.\n",
    "\n",
    "Our training data is going to be drawn from a process,\n",
    "$y = f(x) + \\epsilon$\n",
    "where $f(x)$ is a deterministic function and $\\epsilon\\sim\\mathcal N(0, \\sigma)$ is Gaussian noise with some scale. We're going to choose $f(x) = \\sin(x)$ with $x\\sim\\text{Uniform}(-\\pi, \\pi)$. Our testing data will be $y = f(x)$ for $x$ linearly spaced in $[-\\pi, \\pi]$. Feel free to try out different functions and domains!\n",
    "\n",
    "Since we want to generate our data randomly, we'll need to generate random numbers. Unlike most random number generators that store a global random state, JAX makes the random state explicit (see the [JAX documentation](https://github.com/google/jax#random-numbers-are-different) for more information). Let's therefore start by making some random state using a seed of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8KMAdhnO9i6"
   },
   "outputs": [],
   "source": [
    "key = random.PRNGKey(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvlnKFRindkt"
   },
   "source": [
    "Now let's set up some constants that will define our dataset. In particular, we will use a small training set of 5 points and 50 tests points. Finally, we'll define a noise scale and target function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fan7RcUcngvS"
   },
   "outputs": [],
   "source": [
    "train_points = 5\n",
    "test_points = 50\n",
    "noise_scale = 1e-1\n",
    "\n",
    "target_fn = lambda x: jnp.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YduzMTM-nsEl"
   },
   "source": [
    "Next we generate our training data. We know that we will want to have randomly chosen $x$'s and noise. To generate random numbers in JAX, we have to explicitly evolve the random number state using `random.split` each time we draw a random number.\n",
    "\n",
    "Then we'll want to generate the random inputs, apply the target function, and add the random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQSL0tCjOxVP"
   },
   "outputs": [],
   "source": [
    "key, x_key, y_key = random.split(key, 3)\n",
    "\n",
    "train_xs = random.uniform(x_key, (train_points, 1), minval=-jnp.pi, maxval=jnp.pi)\n",
    "\n",
    "train_ys = target_fn(train_xs)\n",
    "train_ys += noise_scale * random.normal(y_key, (train_points, 1))\n",
    "train = (train_xs, train_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgaBJLDCo2h9"
   },
   "source": [
    "Finally, we want to generate our test data. The $x$'s will be linearly spaced with no noise. Note, we want the inputs to have shape `(N, 1)` instead of `(N,)` since we treat this as a model with one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QRjxH8ZpKuX"
   },
   "outputs": [],
   "source": [
    "test_xs = jnp.linspace(-jnp.pi, jnp.pi, test_points)\n",
    "test_xs = jnp.reshape(test_xs, (test_points, 1))\n",
    "\n",
    "test_ys = target_fn(test_xs)\n",
    "test = (test_xs, test_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi5fGc11puDz"
   },
   "source": [
    "Having generated our data, let's plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 341
    },
    "id": "gUwuP34vPexj",
    "outputId": "0c8bbd6e-5120-4b59-9612-0f09065c0d41"
   },
   "outputs": [],
   "source": [
    "plot_fn(train, test)\n",
    "legend(loc='upper left')\n",
    "finalize_plot((0.85, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu1yKzZcqY2b"
   },
   "source": [
    "What a good looking dataset! Let's train a neural network on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af6ufsPafoFO"
   },
   "source": [
    "## Defining a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luzPB8YvfxhC"
   },
   "source": [
    "The first thing we need to do is define a neural network. We'll start out with a simple fully-connected network using `Erf` nonlinearities. We describe our network using our neural network library that shares syntax and code with JAX's own library called [stax](https://github.com/google/jax#neural-net-building-with-stax). Layers in `jax.example_libraries.stax` are pairs of functions `(init_fn, apply_fn)` where `init_fn(key, input_shape)` draws parameters randomly and `apply_fn(params, xs)` computes outputs of the function for specific inputs. These layers can be composed using `serial` and `parallel` combinators to produce new `(init_fn, apply_fn)` pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uc8XZxuUgTTd"
   },
   "source": [
    "Similarly, layers in `neural_tangents.stax` are triplets of functions `(init_fn, apply_fn, kernel_fn)` where the first two functions are the same as their `stax` equivalent but the third function, `kernel_fn`, computes infinite-width GP kernels corresponding to the layer. Again these layers can be composed using `serial` and `parallel` to build kernels for complicated architectures. Fully-connected layers in `neural_tangents.stax` are created using the `Dense` layer which is defined by,\n",
    "$$z^{l+1}_i = \\frac{\\sigma_w}{\\sqrt{N_{in}}} \\sum_j W_{ij} z^{l}_i + \\sigma_b b_i$$\n",
    "where $W_{ij}, b_i\\sim\\mathcal N(0,1)$ at initialization and $\\sigma_w, \\sigma_b$ sets the scales of the weights and biases respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4u4oqkyEkiAE"
   },
   "outputs": [],
   "source": [
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(512, W_std=1.5, b_std=0.05), stax.Erf(),\n",
    "    stax.Dense(512, W_std=1.5, b_std=0.05), stax.Erf(),\n",
    "    stax.Dense(1, W_std=1.5, b_std=0.05)\n",
    ")\n",
    "\n",
    "apply_fn = jit(apply_fn)\n",
    "kernel_fn = jit(kernel_fn, static_argnames='get')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ87i8-7kpg6"
   },
   "source": [
    "Here the lines `apply_fn = jit(apply_fn)` and `kernel_fn = jit(kernel_fn, static_argnames='get')` use a [JAX feature](https://github.com/google/jax#compilation-with-jit) that compiles functions so that they are executed as single calls to the GPU.\n",
    "\n",
    "Next, let's take several random draws of the parameters of the network and plot what the functions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOz_S-IPlAqZ"
   },
   "outputs": [],
   "source": [
    "prior_draws = []\n",
    "for _ in range(10):\n",
    "  key, net_key = random.split(key)\n",
    "  _, params = init_fn(net_key, (-1, 1))\n",
    "  prior_draws += [apply_fn(params, test_xs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 341
    },
    "id": "FCC6_ER2ng5W",
    "outputId": "ad46f165-e33f-4855-c26b-d1cfbc63ad73"
   },
   "outputs": [],
   "source": [
    "plot_fn(train, test)\n",
    "\n",
    "for p in prior_draws:\n",
    "  plt.plot(test_xs, p, linewidth=3, color=[1, 0.65, 0.65])\n",
    "\n",
    "legend(['train', '$f(x)$', 'random draw'], loc='upper left')\n",
    "\n",
    "finalize_plot((0.85, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUh41cmZl4If"
   },
   "source": [
    "Next we can look at the exact prior over functions in the infinite-width limit using the `kernel_fn`. The kernel function has the signature `kernel = kernel_fn(x_1, x_2)` which computes the kernel between two sets of inputs `x_1` and `x_2`. The `kernel_fn` can compute two different kernels: the NNGP kernel which describes the Bayesian infinite network and the NT kernel which describes how this network evolves under gradient descent. We would like to visualize the standard deviation of the infinite-width function distribution at each $x$. This is given by the diagonal of the NNGP kernel. We compute this now and then plot it compared with the draws above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLo8K96Pl2xw"
   },
   "outputs": [],
   "source": [
    "kernel = kernel_fn(test_xs, test_xs, 'nngp')\n",
    "std_dev = jnp.sqrt(jnp.diag(kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 341
    },
    "id": "60k0Lq3loAz9",
    "outputId": "127e49c1-6df8-4c7c-e156-9358548575c9"
   },
   "outputs": [],
   "source": [
    "plot_fn(train, test)\n",
    "\n",
    "plt.fill_between(\n",
    "    jnp.reshape(test_xs, (-1,)), 2 * std_dev, -2 * std_dev, alpha=0.4)\n",
    "\n",
    "for p in prior_draws:\n",
    "  plt.plot(test_xs, p, linewidth=3, color=[1, 0.65, 0.65])\n",
    "\n",
    "finalize_plot((0.85, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nG_q22a7aJw"
   },
   "source": [
    "## Infinite Width Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3lXqB1t3U9g"
   },
   "source": [
    "We can use the infinite-width GP defined above to perform exact Bayesian inference using the infinite width network. To do this, we will use the function `neural_tangents.predict.gradient_descent_mse_ensemble` that performs this inference exactly. \n",
    "```python\n",
    "predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn, train_xs, train_ys)\n",
    "mean, cov = predict_fn(x_test=test_xs, get='ntk', compute_cov=True)\n",
    "```\n",
    "computes the mean and covariance of the network evaluated on the test points after training. This `predict_fn` function includes two different modes: in \"NNGP\" mode we compute the Bayesian posterior (which is equivalent to gradient descent with all but the last-layer weights frozen), in \"NTK\" mode we compute the distribution of networks after gradient descent training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdTH1Ern4Bt3"
   },
   "source": [
    "We want to do exact Bayesian inference so we'll start off using the \"NNGP\" setting. We will compute and plot these predictions now; we will be concerned with the standard deviation of the predictions on test points which will be given by the diagonal of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeKzXr6Y4Erl"
   },
   "outputs": [],
   "source": [
    "predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn, train_xs,\n",
    "                                                      train_ys, diag_reg=1e-4)\n",
    "\n",
    "nngp_mean, nngp_covariance = predict_fn(x_test=test_xs, get='nngp',\n",
    "                                        compute_cov=True)\n",
    "\n",
    "nngp_mean = jnp.reshape(nngp_mean, (-1,))\n",
    "nngp_std = jnp.sqrt(jnp.diag(nngp_covariance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 341
    },
    "id": "elaecFoO4Erp",
    "outputId": "9122f676-c8b9-4fac-e08a-f83de9ebb87d"
   },
   "outputs": [],
   "source": [
    "plot_fn(train, test)\n",
    "\n",
    "plt.plot(test_xs, nngp_mean, 'r-', linewidth=3)\n",
    "plt.fill_between(\n",
    "    jnp.reshape(test_xs, (-1)),\n",
    "    nngp_mean - 2 * nngp_std,\n",
    "    nngp_mean +  2 * nngp_std,\n",
    "    color='red', alpha=0.2)\n",
    "\n",
    "plt.xlim([-jnp.pi, jnp.pi])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "\n",
    "legend(['Train', 'f(x)', 'Bayesian Inference'], loc='upper left')\n",
    "\n",
    "finalize_plot((0.85, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0bPmNfX42tI"
   },
   "source": [
    "We see that our posterior exactly fits all of the training points as expected. We also see that the there is significant uncertainty in the predictions between the points in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoCJKtoo8b3N"
   },
   "source": [
    "Next, we would like to compute the result of doing gradient descent on our infinite network for an *infinite* amount of time. To do this, we will use the \"NTK\" inference mode. Note that otherwise the call to `predict_fn` looks identical. We will compare the result of true Bayesian inference with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ3xu5lr8MPN"
   },
   "outputs": [],
   "source": [
    "ntk_mean, ntk_covariance = predict_fn(x_test=test_xs, get='ntk',\n",
    "                                      compute_cov=True)\n",
    "\n",
    "ntk_mean = jnp.reshape(ntk_mean, (-1,))\n",
    "ntk_std = jnp.sqrt(jnp.diag(ntk_covariance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 341
    },
    "id": "25U1oRJHq3M9",
    "outputId": "3a3d67ca-dccc-44e5-ae74-ea6a34573f33"
   },
   "outputs": [],
   "source": [
    "plot_fn(train, test)\n",
    "\n",
    "plt.plot(test_xs, nngp_mean, 'r-', linewidth=3)\n",
    "plt.fill_between(\n",
    "    jnp.reshape(test_xs, (-1)),\n",
    "    nngp_mean - 2 * nngp_std,\n",
    "    nngp_mean +  2 * nngp_std,\n",
    "    color='red', alpha=0.2)\n",
    "\n",
    "\n",
    "plt.plot(test_xs, ntk_mean, 'b-', linewidth=3)\n",
    "plt.fill_between(\n",
    "    jnp.reshape(test_xs, (-1)),\n",
    "    ntk_mean - 2 * ntk_std,\n",
    "    ntk_mean +  2 * ntk_std,\n",
    "    color='blue', alpha=0.2)\n",
    "\n",
    "plt.xlim([-jnp.pi, jnp.pi])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "\n",
    "legend(['Train', 'f(x)', 'Bayesian Inference', 'Gradient Descent'],\n",
    "       loc='upper left')\n",
    "\n",
    "finalize_plot((0.85, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwV7HILPsNrz"
   },
   "source": [
    "We see that while the result of gradient descent and bayesian inference are similar, they are not identical.\n",
    "\n",
    "Not only can we do inference at infinite times, but we can also perform finite-time inference. We will use this to predict the mean of the train and test losses over the course of training. To compute the mean MSE loss, we need to access the mean and variance of our networks predictions as a function of time. To do this, we call our `predict_fn` function with finite times `t` (as opposed to using the default value `t=None` earlier, considered as infinite time). Note that `predict` can act on both scalar and array values, so we simply invoke the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNrvoLXx7Jbp"
   },
   "outputs": [],
   "source": [
    "ts = jnp.arange(0, 10 ** 3, 10 ** -1)\n",
    "ntk_train_loss_mean = loss_fn(predict_fn, train_ys, ts)\n",
    "ntk_test_loss_mean = loss_fn(predict_fn, test_ys, ts, test_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 338
    },
    "id": "0iwoN0LDR28N",
    "outputId": "5116d0e4-9757-4c84-f1e7-6dffb26d8fdf"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.loglog(ts, ntk_train_loss_mean, linewidth=3)\n",
    "plt.loglog(ts, ntk_test_loss_mean, linewidth=3)\n",
    "format_plot('Step', 'Loss')\n",
    "legend(['Infinite Train', 'Infinite Test'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plot_fn(train, None)\n",
    "\n",
    "plt.plot(test_xs, ntk_mean, 'b-', linewidth=3)\n",
    "plt.fill_between(\n",
    "    jnp.reshape(test_xs, (-1)),\n",
    "    ntk_mean - 2 * ntk_std,\n",
    "    ntk_mean +  2 * ntk_std,\n",
    "    color='blue', alpha=0.2)\n",
    "\n",
    "legend(\n",
    "    ['Train', 'Infinite Network'],\n",
    "    loc='upper left')\n",
    "\n",
    "finalize_plot((1.5, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98jPUzUEOXgy"
   },
   "source": [
    "## Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9Zu-EXywF22"
   },
   "source": [
    "We will now compare the results of gradient descent GP-inference computed above to the result of training an ensemble of finite width neural networks. We first train a single network drawn from the prior and then we will show how to generalize this to an ensemble. To do this we use JAX's gradient descent [optimizer](https://github.com/google/jax#first-order-optimization). Optimizers are described by a triple of functions `(init_fn, update_fn, get_params)`. Here, `init_fn(params)` takes an initial set of parameters and returns an optimizer state that can include extra information (like the velocity in the `momentum` optimizer). `opt_update(step, grads, state)` takes a new state and updates it using gradients. Finally, `get_params(state)` returns the parameters for a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gG36dJv_VUaR"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "training_steps = 10000\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.sgd(learning_rate)\n",
    "opt_update = jit(opt_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KBX5kpiUKHi"
   },
   "source": [
    "Next we need to define a loss and a gradient of the loss. We'll use an MSE loss. The function `grad` is another [JAX function](https://github.com/google/jax#automatic-differentiation-with-grad) that takes a function and returns a new function that computes its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTGamMDaUTu5"
   },
   "outputs": [],
   "source": [
    "loss = jit(lambda params, x, y: 0.5 * jnp.mean((apply_fn(params, x) - y) ** 2))\n",
    "grad_loss = jit(lambda state, x, y: grad(loss)(get_params(state), x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1plIKII_USSM"
   },
   "source": [
    "Now we want to actually train the network. To do this we just initialize the optimizer state and then update it however many times we want. We'll record the train and test loss after each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNRGGcNLUE2l"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "for i in range(training_steps):\n",
    "  opt_state = opt_update(i, grad_loss(opt_state, *train), opt_state)\n",
    "\n",
    "  train_losses += [loss(get_params(opt_state), *train)]\n",
    "  test_losses += [loss(get_params(opt_state), *test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7jQRWzRy4OF"
   },
   "source": [
    "Finally, lets plot the loss over the course of training and the function after training compared with our GP inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 338
    },
    "id": "KcqvB9M-WZiv",
    "outputId": "579b4b84-868f-4f05-bb2d-2ee4357c5a32"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.loglog(ts, ntk_train_loss_mean, linewidth=3)\n",
    "plt.loglog(ts, ntk_test_loss_mean, linewidth=3)\n",
    "\n",
    "plt.loglog(ts, train_losses, 'k-', linewidth=2)\n",
    "plt.loglog(ts, test_losses, 'k-', linewidth=2)\n",
    "\n",
    "format_plot('Step', 'Loss')\n",
    "legend(['Infinite Train', 'Infinite Test', 'Finite'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plot_fn(train, None)\n",
    "\n",
    "plt.plot(test_xs, ntk_mean, 'b-', linewidth=3)\n",
    "plt.fill_between(\n",
    "    jnp.reshape(test_xs, (-1)),\n",
    "    ntk_mean - 2 * ntk_std,\n",
    "    ntk_mean +  2 * ntk_std,\n",
    "    color='blue', alpha=0.2)\n",
    "\n",
    "plt.plot(test_xs, apply_fn(get_params(opt_state), test_xs), 'k-', linewidth=2)\n",
    "\n",
    "legend(\n",
    "    ['Train', 'Infinite Network', 'Finite Network'],\n",
    "    loc='upper left')\n",
    "\n",
    "finalize_plot((1.5, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcLr3DjtaCzE"
   },
   "source": [
    "## Training an Ensemble of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC-gh5Gez7xd"
   },
   "source": [
    "The draw above certainly seems consistent with exact inference. However, as discussed above to make a more quantitative comparison we want to train an ensemble of finite-width networks. We could use a for-loop to loop over all the different instantiations that we wanted to evaluate. However, it is more convenient and efficient to use another [JAX feature](https://github.com/google/jax#auto-vectorization-with-vmap) called `vmap`. `vmap` takes a function and vectorizes it over a batch dimension. In this case, we're going to add a batch dimension to our training loop so that we train a whole batch of neural networks at once. To do that, let's first wrap our whole training loop in a function. The function will take a random state and train a network based on that random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-w2MxZz1yO1"
   },
   "outputs": [],
   "source": [
    "def train_network(key):\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "\n",
    "  _, params = init_fn(key, (-1, 1))\n",
    "  opt_state = opt_init(params)\n",
    "\n",
    "  for i in range(training_steps):\n",
    "    train_losses += [jnp.reshape(loss(get_params(opt_state), *train), (1,))]\n",
    "    test_losses += [jnp.reshape(loss(get_params(opt_state), *test), (1,))]\n",
    "    opt_state = opt_update(i, grad_loss(opt_state, *train), opt_state)\n",
    "\n",
    "  train_losses = jnp.concatenate(train_losses)\n",
    "  test_losses = jnp.concatenate(test_losses)\n",
    "  return get_params(opt_state), train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVrlZGhh2Ck-"
   },
   "source": [
    "We can test it to make sure that we get a trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83ld3fTm2MiI"
   },
   "outputs": [],
   "source": [
    "# test {\"skip\": true}\n",
    "params, train_loss, test_loss = train_network(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 338
    },
    "id": "_8XLiCZrvCjQ",
    "outputId": "a15cb757-bdb8-4c56-c8a9-f503316b5054"
   },
   "outputs": [],
   "source": [
    "# test {\"skip\": true}\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.loglog(ts, ntk_train_loss_mean, linewidth=3)\n",
    "plt.loglog(ts, ntk_test_loss_mean, linewidth=3)\n",
    "\n",
    "plt.loglog(ts, train_loss, 'k-', linewidth=2)\n",
    "plt.loglog(ts, test_loss, 'k-', linewidth=2)\n",
    "\n",
    "format_plot('Step', 'Loss')\n",
    "legend(['Train', 'Test', 'Finite'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plot_fn(train, None)\n",
    "\n",
    "plt.plot(test_xs, ntk_mean, 'b-', linewidth=3)\n",
    "plt.fill_between(\n",
    "    jnp.reshape(test_xs, (-1)),\n",
    "    ntk_mean - 2 * ntk_std,\n",
    "    ntk_mean +  2 * ntk_std,\n",
    "    color='blue', alpha=0.2)\n",
    "\n",
    "plt.plot(test_xs, apply_fn(params, test_xs), 'k-', linewidth=2)\n",
    "\n",
    "legend(['Train', 'Infinite Network', 'Finite Network'], loc='upper left')\n",
    "\n",
    "finalize_plot((1.5, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ar-Zs2LW2u7P"
   },
   "source": [
    "Now, to train an ensemble we just have to apply `vmap` to `train_network`. The resulting function will take a vector of random states and will train one network for each random state in the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJHWdE6V21i1"
   },
   "outputs": [],
   "source": [
    "# test {\"skip\": true}\n",
    "ensemble_size = 100\n",
    "ensemble_key = random.split(key, ensemble_size)\n",
    "params, train_loss, test_loss = vmap(train_network)(ensemble_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYeAAo2v3gPn"
   },
   "source": [
    "Let's plot the empirical standard deviation in the loss over the course of training as well as well as for the function after gradient descent compared with the exact inference above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 338
    },
    "id": "CUG5v8jQvhqU",
    "outputId": "4c4969f4-8045-47d8-bcb7-613d17525aaf"
   },
   "outputs": [],
   "source": [
    "# test {\"skip\": true}\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "mean_train_loss = jnp.mean(train_loss, axis=0)\n",
    "mean_test_loss = jnp.mean(test_loss, axis=0)\n",
    "\n",
    "plt.loglog(ts, ntk_train_loss_mean, linewidth=3)\n",
    "plt.loglog(ts, ntk_test_loss_mean, linewidth=3)\n",
    "\n",
    "plt.loglog(ts, mean_train_loss, 'k-', linewidth=2)\n",
    "plt.loglog(ts, mean_test_loss, 'k-', linewidth=2)\n",
    "\n",
    "plt.xlim([10 ** 0, 10 ** 3])\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "format_plot('Step', 'Loss')\n",
    "\n",
    "legend(['Infinite Train', 'Infinite Test', 'Finite Ensemble'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plot_fn(train, None)\n",
    "\n",
    "plt.plot(test_xs, ntk_mean, 'b-', linewidth=3)\n",
    "plt.fill_between(\n",
    "    jnp.reshape(test_xs, (-1)),\n",
    "    ntk_mean - 2 * ntk_std,\n",
    "    ntk_mean +  2 * ntk_std,\n",
    "    color='blue', alpha=0.2)\n",
    "\n",
    "ensemble_fx = vmap(apply_fn, (0, None))(params, test_xs)\n",
    "\n",
    "mean_fx = jnp.reshape(jnp.mean(ensemble_fx, axis=0), (-1,))\n",
    "std_fx = jnp.reshape(jnp.std(ensemble_fx, axis=0), (-1,))\n",
    "\n",
    "plt.plot(test_xs, mean_fx - 2 * std_fx, 'k--', label='_nolegend_')\n",
    "plt.plot(test_xs, mean_fx + 2 * std_fx, 'k--', label='_nolegend_')\n",
    "plt.plot(test_xs, mean_fx, linewidth=2, color='black')\n",
    "\n",
    "legend(['Train', 'Infinite Network', 'Finite Ensemble'], loc='upper left')\n",
    "\n",
    "plt.xlim([-jnp.pi, jnp.pi])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "\n",
    "format_plot('$x$', '$f$')\n",
    "finalize_plot((1.5, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvqFKuxY5pWx"
   },
   "source": [
    "We see pretty nice agreement between exact inference of the infinite-width networks and the result of training an ensemble! Note that we do see some deviations in the training loss at the end of training. This is ameliorated by using a wider network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c39JDz2tD2di"
   },
   "source": [
    "## Playing Around with the Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1i0fwz3EfR8"
   },
   "source": [
    "To demonstrate the ease of specifying more exotic architecture, can try to reproduce the above results with different choices of architecture. For fn, let's see what happens if we add residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txOpM97RE_EE"
   },
   "outputs": [],
   "source": [
    "ResBlock = stax.serial(\n",
    "    stax.FanOut(2),\n",
    "    stax.parallel(\n",
    "        stax.serial(\n",
    "            stax.Erf(),\n",
    "            stax.Dense(512, W_std=1.1, b_std=0),\n",
    "        ),\n",
    "        stax.Identity()\n",
    "    ),\n",
    "    stax.FanInSum()\n",
    ")\n",
    "\n",
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(512, W_std=1, b_std=0),\n",
    "    ResBlock, ResBlock, stax.Erf(),\n",
    "    stax.Dense(1, W_std=1.5, b_std=0)\n",
    ")\n",
    "\n",
    "apply_fn = jit(apply_fn)\n",
    "kernel_fn = jit(kernel_fn, static_argnames='get')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eli8h43jGc2n"
   },
   "source": [
    "Given this new architecture, let's train a new ensemble of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xz1cWyCE39Z"
   },
   "outputs": [],
   "source": [
    "# test {\"skip\": true}\n",
    "ensemble_size = 100\n",
    "learning_rate = 0.1\n",
    "ts = jnp.arange(0, 10 ** 3, learning_rate)\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.sgd(learning_rate)\n",
    "opt_update = jit(opt_update)\n",
    "\n",
    "key, = random.split(key, 1)\n",
    "ensemble_key = random.split(key, ensemble_size)\n",
    "params, train_loss, test_loss = vmap(train_network)(ensemble_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9ttNWWSGgBV"
   },
   "source": [
    "Finally, let's repeat our NTK-GP inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEMTCSGpFcDL"
   },
   "outputs": [],
   "source": [
    "# test {\"skip\": true}\n",
    "predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn, train_xs,\n",
    "                                                      train_ys, diag_reg=1e-4)\n",
    "ntk_mean, ntk_var = predict_fn(x_test=test_xs, get='ntk', compute_cov=True)\n",
    "\n",
    "ntk_mean = jnp.reshape(ntk_mean, (-1,))\n",
    "ntk_std = jnp.sqrt(jnp.diag(ntk_var))\n",
    "\n",
    "ntk_train_loss_mean = loss_fn(predict_fn, train_ys, ts)\n",
    "ntk_test_loss_mean = loss_fn(predict_fn, test_ys, ts, test_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uLi8wdcGjaU"
   },
   "source": [
    "Now let's draw the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 338
    },
    "id": "nlOJ4QuK0-Ss",
    "outputId": "d828770f-c37b-4371-e742-8473f7e6a489"
   },
   "outputs": [],
   "source": [
    "# test {\"skip\": true}\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "mean_train_loss = jnp.mean(train_loss, axis=0)\n",
    "mean_test_loss = jnp.mean(test_loss, axis=0)\n",
    "\n",
    "plt.loglog(ts, ntk_train_loss_mean, linewidth=3)\n",
    "plt.loglog(ts, ntk_test_loss_mean, linewidth=3)\n",
    "\n",
    "plt.loglog(ts, mean_train_loss, 'k-', linewidth=2)\n",
    "plt.loglog(ts, mean_test_loss, 'k-', linewidth=2)\n",
    "\n",
    "plt.xlim([10 ** 0, 10 ** 3])\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "format_plot('Step', 'Loss')\n",
    "\n",
    "legend(['Infinite Train', 'Infinite Test', 'Finite Ensemble'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plot_fn(train, None)\n",
    "\n",
    "plt.plot(test_xs, ntk_mean, 'b-', linewidth=3)\n",
    "plt.fill_between(\n",
    "    jnp.reshape(test_xs, (-1)),\n",
    "    ntk_mean - 2 * ntk_std,\n",
    "    ntk_mean +  2 * ntk_std,\n",
    "    color='blue', alpha=0.2)\n",
    "\n",
    "ensemble_fx = vmap(apply_fn, (0, None))(params, test_xs)\n",
    "\n",
    "mean_fx = jnp.reshape(jnp.mean(ensemble_fx, axis=0), (-1,))\n",
    "std_fx = jnp.reshape(jnp.std(ensemble_fx, axis=0), (-1,))\n",
    "\n",
    "plt.plot(test_xs, mean_fx - 2 * std_fx, 'k--', label='_nolegend_')\n",
    "plt.plot(test_xs, mean_fx + 2 * std_fx, 'k--', label='_nolegend_')\n",
    "plt.plot(test_xs, mean_fx, linewidth=2, color='black')\n",
    "\n",
    "legend(['Train', 'Infinite Network', 'Finite Ensemble'], loc='upper left')\n",
    "\n",
    "plt.xlim([-jnp.pi, jnp.pi])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "\n",
    "format_plot('$x$', '$f$')\n",
    "finalize_plot((1.5, 0.6))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Neural Tangents Cookbook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
