{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfec7aa-f386-422e-a965-3f15aa2e3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import neural_tangents as nt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, jit, jacfwd, jacrev, lax, random, vmap\n",
    "from jax.example_libraries import optimizers\n",
    "from neural_tangents import stax\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c67464-d868-4ace-bfc2-fc6c52de9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.colormaps.get_cmap('tab20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc9299-fb90-43b4-97e4-2c24b36bcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3(a) and 3(b). Sweep over alpha and epsilon.\n",
    "def phi(z, eps = 0.25):\n",
    "    return z + 0.5*eps*z**2\n",
    "\n",
    "def NN_func2(params, X, alpha, eps=0.25):\n",
    "    a, W = params\n",
    "\n",
    "    D = W.shape[1]\n",
    "    N = a.shape[0]\n",
    "\n",
    "    h = W @ X / jnp.sqrt(D)\n",
    "    f = alpha * jnp.mean( phi(h, eps = eps), axis = 0)\n",
    "    return f\n",
    "\n",
    "def target_fn(beta, X):\n",
    "    return (X.T @ beta / jnp.sqrt(D))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00064ad-ea80-4448-9fc5-8a0af73c957e",
   "metadata": {},
   "source": [
    "# Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa6e38-cbd7-436a-8541-b99afca8b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Xt.shape, beta.shape, params[0].shape, params[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc86b0-4170-496b-90a3-8b3633bb136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 100\n",
    "P = 550\n",
    "N = 500\n",
    "ntk_interval = 100 \n",
    "\n",
    "X = random.normal(random.PRNGKey(0), (D,P))\n",
    "Xt = random.normal(random.PRNGKey(1), (D,1000))\n",
    "beta = random.normal(random.PRNGKey(2), (D,))\n",
    "\n",
    "y = target_fn(beta, X)\n",
    "yt = target_fn(beta, Xt)\n",
    "\n",
    "a = random.normal(random.PRNGKey(0), (N, ))\n",
    "W = random.normal(random.PRNGKey(0), (N, D))\n",
    "params = [a, W]\n",
    "\n",
    "eps = 0.25\n",
    "eta = 0.5 * N\n",
    "lamb = 0.0\n",
    "opt_init, opt_update, get_params = optimizers.sgd(eta)\n",
    "\n",
    "alphas = [2**(-5),0.25,0.5,1.0,2.0,4.0,8.0,16,32]\n",
    "\n",
    "all_tr_losses = []\n",
    "all_te_losses = []\n",
    "all_acc_tr = []\n",
    "all_acc_te = []\n",
    "\n",
    "param_movement = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    def nn_wrapper(params, X):\n",
    "        return NN_func2(params, X, alpha=alpha, eps=0.25)\n",
    "    \n",
    "    ntk_fn = nt.empirical_ntk_fn(NN_func2_wrapped, vmap_axes=0, trace_axes=())\n",
    "    K_0 = ntk_fn(Xt, None, params)\n",
    "    np.save(f\"kernels_alpha_sweep/k_0\", K_0)\n",
    "\n",
    "    opt_state = opt_init(params)\n",
    "    loss_fn = jit(lambda p, X, y: jnp.mean( ( NN_func2(p, X, alpha) - y )**2 / alpha**2 ))\n",
    "    acc_fn = jit(lambda p, X, y: jnp.mean( ( y * NN_func2(p, X,alpha)) > 0.0 ))\n",
    "    reg_loss = jit(lambda p, X, y: loss_fn(p,X,y) + lamb / alpha * optimizers.l2_norm(p)**2 )\n",
    "\n",
    "    grad_loss = jit(grad(reg_loss,0))\n",
    "\n",
    "    tr_losses = []\n",
    "    te_losses = []\n",
    "    tr_acc = []\n",
    "    te_acc = []\n",
    "    for t in range(60000):\n",
    "        opt_state = opt_update(t, grad_loss(get_params(opt_state), X, y), opt_state)\n",
    "        if t % 2 == 0:\n",
    "            train_loss = alpha**2*loss_fn(get_params(opt_state), X, y)\n",
    "            test_loss = alpha**2*loss_fn(get_params(opt_state), Xt, yt)\n",
    "            tr_losses += [train_loss]\n",
    "            te_losses += [test_loss]\n",
    "            tr_acc += [ acc_fn(get_params(opt_state), X, y) ]\n",
    "            te_acc += [ acc_fn(get_params(opt_state), Xt, yt) ]\n",
    "            sys.stdout.write(f'\\r t: {t} | train loss: {train_loss} | test loss: {test_loss}')\n",
    "        if t % 10000 == 0:\n",
    "            print(\" \")\n",
    "\n",
    "        if t % ntk_interval == 0:\n",
    "            K_test = ntk_fn(Xt.T, None, params)\n",
    "            np.save(f\"kernels_alpha_sweep/k_{t}\", K_test) \n",
    "    all_tr_losses += [tr_losses]\n",
    "    all_te_losses += [te_losses]\n",
    "    all_acc_tr += [tr_acc]\n",
    "    all_acc_te += [te_acc]\n",
    "\n",
    "    paramsf = get_params(opt_state)\n",
    "    dparam = (jnp.sum((paramsf[0]-params[0])**2) + jnp.sum((paramsf[1]-params[1])**2)) / ( jnp.sum( params[0]**2 ) + jnp.sum(params[1]**2) )\n",
    "    param_movement += [  dparam ]\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.figure()\n",
    "for i,alpha in enumerate(alphas[:-1]):\n",
    "    print(alpha)\n",
    "    plt.plot(jnp.linspace(1,len(all_tr_losses[i]),len(all_tr_losses[i])), jnp.array(all_tr_losses[i]) / all_tr_losses[i][0], '--',  color = f'C{i}')\n",
    "    plt.plot(jnp.linspace(1,len(all_tr_losses[i]),len(all_tr_losses[i])), jnp.array(all_te_losses[i]) / all_te_losses[i][0],  color = f'C{i}', label = r'$\\alpha = 2^{%0.0f}$' % jnp.log2(alpha))\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'$t$',fontsize = 20)\n",
    "plt.ylabel('Loss',fontsize = 20)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c6f20-6dc0-48b0-9e77-fb052ef321f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Weight Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84395a76-cc41-47cb-a9e9-ab668ac15786",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_norms = [0.125,0.25,0.5,1.0,2.0]\n",
    "alpha = 1.0\n",
    "\n",
    "eta = 0.5 * N\n",
    "lamb = 0.0\n",
    "\n",
    "all_tr_losses_w = []\n",
    "all_te_losses_w = []\n",
    "all_acc_tr_w = []\n",
    "all_acc_te_w = []\n",
    "\n",
    "param_movement_w = []\n",
    "\n",
    "for i, wscale in enumerate(weight_norms):\n",
    "    a = wscale * random.normal(random.PRNGKey(0), (N, ))\n",
    "    W = wscale * random.normal(random.PRNGKey(0), (N, D))\n",
    "    params = [a, W]\n",
    "\n",
    "    opt_init, opt_update, get_params = optimizers.sgd( eta / wscale**2 )\n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "\n",
    "    loss_fn = jit(lambda p, X, y: jnp.mean( ( NN_func2(p, X,alpha)- NN_func2(params,X,alpha) - y )**2 / alpha**2 ))\n",
    "    acc_fn = jit(lambda p, X, y: jnp.mean( ( y * ( NN_func2(p, X,alpha)- NN_func2(params,X,alpha)) ) > 0.0 ))\n",
    "    reg_loss = jit(lambda p, X, y: loss_fn(p,X,y) + lamb / alpha * optimizers.l2_norm(p)**2 )\n",
    "\n",
    "    grad_loss = jit(grad(reg_loss,0))\n",
    "\n",
    "    tr_losses = []\n",
    "    te_losses = []\n",
    "    tr_acc = []\n",
    "    te_acc = []\n",
    "    for t in range(50000):\n",
    "        opt_state = opt_update(t, grad_loss(get_params(opt_state), X, y), opt_state)\n",
    "\n",
    "        if t % 2 == 0:\n",
    "            train_loss = alpha**2*loss_fn(get_params(opt_state), X, y)\n",
    "            test_loss = alpha**2*loss_fn(get_params(opt_state), Xt, yt)\n",
    "            tr_losses += [train_loss]\n",
    "            te_losses += [test_loss]\n",
    "            tr_acc += [ acc_fn(get_params(opt_state), X, y) ]\n",
    "            te_acc += [ acc_fn(get_params(opt_state), Xt, yt) ]\n",
    "            sys.stdout.write(f'\\r t: {t} | train loss: {train_loss} | test loss: {test_loss}')\n",
    "        if t % 10000 == 0:\n",
    "            print(\" \")\n",
    "\n",
    "    all_tr_losses_w += [tr_losses]\n",
    "    all_te_losses_w += [te_losses]\n",
    "    all_acc_tr_w += [tr_acc]\n",
    "    all_acc_te_w += [te_acc]\n",
    "\n",
    "    paramsf = get_params(opt_state)\n",
    "    dparam = (jnp.sum((paramsf[0]-params[0])**2) + jnp.sum((paramsf[1]-params[1])**2)) / ( jnp.sum(params[0]**2) + jnp.sum(params[1]**2) )\n",
    "    param_movement_w += [  dparam ]\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.figure()\n",
    "for i, wscale in enumerate(weight_norms):\n",
    "    print(alpha)\n",
    "    plt.plot(\n",
    "        jnp.linspace(1,len(all_tr_losses_w[i]),len(all_tr_losses_w[i])), \n",
    "        jnp.array(all_tr_losses_w[i]) / all_tr_losses_w[i][0], \n",
    "        '--',  \n",
    "        color = f'C{i}'\n",
    "    )\n",
    "    plt.plot(\n",
    "        jnp.linspace(1,len(all_tr_losses_w[i]),len(all_tr_losses_w[i])), \n",
    "        jnp.array(all_te_losses_w[i]) / all_te_losses_w[i][0],  \n",
    "        color = f'C{i}', \n",
    "        label = r'$\\sigma = 2^{%0.0f}$' % jnp.log2(wscale)\n",
    "    )\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('t',fontsize = 20)\n",
    "plt.ylabel('Loss',fontsize = 20)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f1aff-3c95-41c2-b397-cd987e9b6f2f",
   "metadata": {},
   "source": [
    "# Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3e76c-d67d-414f-a6f1-e3d0202c2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 100\n",
    "P = 550\n",
    "N = 500\n",
    "\n",
    "X = random.normal(random.PRNGKey(0), (D,P))\n",
    "Xt = random.normal(random.PRNGKey(1), (D,1000))\n",
    "beta = random.normal(random.PRNGKey(2), (D,))\n",
    "\n",
    "y = target_fn(beta, X)\n",
    "yt = target_fn(beta,Xt)\n",
    "\n",
    "a = random.normal(random.PRNGKey(0), (N, ))\n",
    "W = random.normal(random.PRNGKey(0), (N, D))\n",
    "params = [a, W]\n",
    "\n",
    "\n",
    "eta = 0.5 * N\n",
    "lamb = 0.0\n",
    "opt_init, opt_update, get_params = optimizers.sgd(eta)\n",
    "\n",
    "epsilons = [2**(-2),2**(-1),1,2,4]\n",
    "alpha=1\n",
    "\n",
    "all_tr_losses_eps = []\n",
    "all_te_losses_eps = []\n",
    "all_acc_tr_eps = []\n",
    "all_acc_te_eps = []\n",
    "\n",
    "param_movement = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "    loss_fn = jit(lambda p, X, y: jnp.mean( ( NN_func2(p, X,alpha,eps) - y )**2 / alpha**2 ))\n",
    "    acc_fn = jit(lambda p, X, y: jnp.mean( ( y * NN_func2(p, X,alpha,eps) ) > 0.0 ))\n",
    "    reg_loss = jit(lambda p, X, y: loss_fn(p,X,y) + lamb / alpha * optimizers.l2_norm(p)**2 )\n",
    "\n",
    "    grad_loss = jit(grad(reg_loss,0))\n",
    "\n",
    "    tr_losses = []\n",
    "    te_losses = []\n",
    "    tr_acc = []\n",
    "    te_acc = []\n",
    "    for t in range(60000):\n",
    "        opt_state = opt_update(t, grad_loss(get_params(opt_state), X, y), opt_state)\n",
    "\n",
    "        if t % 2 == 0:\n",
    "            train_loss = alpha**2*loss_fn(get_params(opt_state), X, y)\n",
    "            test_loss = alpha**2*loss_fn(get_params(opt_state), Xt, yt)\n",
    "            tr_losses += [train_loss]\n",
    "            te_losses += [test_loss]\n",
    "            tr_acc += [ acc_fn(get_params(opt_state), X, y) ]\n",
    "            te_acc += [ acc_fn(get_params(opt_state), Xt, yt) ]\n",
    "            sys.stdout.write(f'\\r t: {t} | train loss: {train_loss} | test loss: {test_loss}')\n",
    "        if t % 10000 == 0:\n",
    "            print(\" \")\n",
    "\n",
    "    all_tr_losses_eps += [tr_losses]\n",
    "    all_te_losses_eps += [te_losses]\n",
    "    all_acc_tr_eps += [tr_acc]\n",
    "    all_acc_te_eps += [te_acc]\n",
    "\n",
    "    paramsf = get_params(opt_state)\n",
    "    dparam = (jnp.sum((paramsf[0]-params[0])**2) + jnp.sum((paramsf[1]-params[1])**2)) / ( jnp.sum( params[0]**2 ) + jnp.sum(params[1]**2) )\n",
    "    param_movement += [  dparam ]\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.figure()\n",
    "for i,alpha in enumerate(alphas[:-1]):\n",
    "    print(alpha)\n",
    "    plt.plot(jnp.linspace(1,len(all_tr_losses[i]),len(all_tr_losses[i])), jnp.array(all_tr_losses[i]) / all_tr_losses[i][0], '--',  color = f'C{i}')\n",
    "    plt.plot(jnp.linspace(1,len(all_tr_losses[i]),len(all_tr_losses[i])), jnp.array(all_te_losses[i]) / all_te_losses[i][0],  color = f'C{i}', label = r'$\\alpha = 2^{%0.0f}$' % jnp.log2(alpha))\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'$t$',fontsize = 20)\n",
    "plt.ylabel('Loss',fontsize = 20)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1097040-0b3e-44cc-aa94-0a601b777abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
