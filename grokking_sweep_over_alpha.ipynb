{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bfec7aa-f386-422e-a965-3f15aa2e3128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 23:18:27.276920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747106307.328215  188854 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747106307.344921  188854 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747106307.459602  188854 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747106307.459617  188854 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747106307.459619  188854 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747106307.459620  188854 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import neural_tangents as nt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, jit, jacfwd, jacrev, lax, random, vmap\n",
    "from jax.example_libraries import optimizers\n",
    "from neural_tangents import stax\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c67464-d868-4ace-bfc2-fc6c52de9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.colormaps.get_cmap('tab20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cfc9299-fb90-43b4-97e4-2c24b36bcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3(a) and 3(b). Sweep over alpha and epsilon.\n",
    "def phi(z, eps = 0.25):\n",
    "    return z + 0.5*eps*z**2\n",
    "\n",
    "def NN_func2(params, X, alpha, eps=0.25):\n",
    "    a, W = params\n",
    "\n",
    "    D = W.shape[1]\n",
    "    N = a.shape[0]\n",
    "\n",
    "    h = W @ X / jnp.sqrt(D)\n",
    "    f = alpha * jnp.mean( phi(h, eps = eps), axis = 0)\n",
    "    return f\n",
    "\n",
    "def target_fn(beta, X):\n",
    "    return (X.T @ beta / jnp.sqrt(D))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f50266aa-de51-4bcc-8991-3b492d4476b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobenius(x):\n",
    "    eigenval, eigenvec = jax.numpy.linalg.eigh(x)\n",
    "    frobenius_norm = jnp.sqrt(np.sum(eigenval**2))\n",
    "    return frobenius_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00064ad-ea80-4448-9fc5-8a0af73c957e",
   "metadata": {},
   "source": [
    "# Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "204ad158-b192-42e1-8e92-529a1589e2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 550), (100, 1000))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49203609-ecd4-4a52-8258-b0028b47ce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "  t: 8000 | train loss: 9.929312909662258e-06 | test loss: 0.00046128538087941706 |frob ||kt-k0||_2: 0.003107063937932253 \n",
      "  t: 18000 | train loss: 2.5359890969411936e-06 | test loss: 0.00013132266758475453 |frob ||kt-k0||_2: 0.003123149974271655 \n",
      "  t: 20000 | train loss: 2.119992359439493e-06 | test loss: 0.00011106090096291155 |frob ||kt-k0||_2: 0.00312460376881063"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m     te_losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [test_loss]\n\u001b[1;32m     57\u001b[0m     tr_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [ acc_fn(get_params(opt_state), X, y) ]\n\u001b[0;32m---> 58\u001b[0m     te_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [ \u001b[43macc_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myt\u001b[49m\u001b[43m)\u001b[49m ]\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# sys.stdout.write(f'\\r t: {t} | train loss: {train_loss} | test loss: {test_loss}')\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "D = 100\n",
    "P = 550\n",
    "N = 500\n",
    "ntk_interval = 100 \n",
    "\n",
    "X = random.normal(random.PRNGKey(0), (D,P))\n",
    "Xt = random.normal(random.PRNGKey(1), (D,1000))\n",
    "beta = random.normal(random.PRNGKey(2), (D,))\n",
    "\n",
    "y = target_fn(beta, X)\n",
    "yt = target_fn(beta, Xt)\n",
    "\n",
    "a = random.normal(random.PRNGKey(0), (N, ))\n",
    "W = random.normal(random.PRNGKey(0), (N, D))\n",
    "params = [a, W]\n",
    "\n",
    "eps = 0.25\n",
    "eta = 0.5 * N\n",
    "lamb = 0.0\n",
    "opt_init, opt_update, get_params = optimizers.sgd(eta)\n",
    "\n",
    "alphas = [2**(-5)] #,0.25,0.5,1.0,2.0,4.0,8.0,16,32]\n",
    "\n",
    "all_tr_losses = []\n",
    "all_te_losses = []\n",
    "all_acc_tr = []\n",
    "all_acc_te = []\n",
    "\n",
    "param_movement = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    def nn_wrapper(params, X):\n",
    "        return NN_func2(params, X.T, alpha=alpha, eps=0.25)\n",
    "    \n",
    "    ntk_fn = nt.empirical_ntk_fn(nn_wrapper, vmap_axes=0, trace_axes=())\n",
    "    K_0 = ntk_fn(Xt.T, None, params)\n",
    "    np.save(f\"kernels_alpha_sweep/k_0\", K_0)\n",
    "\n",
    "    opt_state = opt_init(params)\n",
    "    loss_fn = jit(lambda p, X, y: jnp.mean( ( NN_func2(p, X, alpha) - y )**2 / alpha**2 ))\n",
    "    acc_fn = jit(lambda p, X, y: jnp.mean( ( y * NN_func2(p, X,alpha)) > 0.0 ))\n",
    "    reg_loss = jit(lambda p, X, y: loss_fn(p,X,y) + lamb / alpha * optimizers.l2_norm(p)**2 )\n",
    "\n",
    "    grad_loss = jit(grad(reg_loss,0))\n",
    "\n",
    "    tr_losses = []\n",
    "    te_losses = []\n",
    "    tr_acc = []\n",
    "    te_acc = []\n",
    "    for t in range(60000):\n",
    "        opt_state = opt_update(t, grad_loss(get_params(opt_state), X, y), opt_state)\n",
    "        if t % 2 == 0:\n",
    "            train_loss = alpha**2*loss_fn(get_params(opt_state), X, y)\n",
    "            test_loss = alpha**2*loss_fn(get_params(opt_state), Xt, yt)\n",
    "            tr_losses += [train_loss]\n",
    "            te_losses += [test_loss]\n",
    "            tr_acc += [ acc_fn(get_params(opt_state), X, y) ]\n",
    "            te_acc += [ acc_fn(get_params(opt_state), Xt, yt) ]\n",
    "            # sys.stdout.write(f'\\r t: {t} | train loss: {train_loss} | test loss: {test_loss}')\n",
    "        if t % 10000 == 0:\n",
    "            print(\" \")\n",
    "\n",
    "        if t % ntk_interval == 0:\n",
    "            K_test = ntk_fn(Xt.T, None, get_params(opt_state))\n",
    "            sys.stdout.write(f'\\r  t: {t} | train loss: {train_loss} | test loss: {test_loss} |frob ||kt-k0||_2: {frobenius(K_test-K_0)}')\n",
    "            # np.save(f\"kernels_alpha_sweep/k_{t}\", K_test) \n",
    "    all_tr_losses += [tr_losses]\n",
    "    all_te_losses += [te_losses]\n",
    "    all_acc_tr += [tr_acc]\n",
    "    all_acc_te += [te_acc]\n",
    "\n",
    "    paramsf = get_params(opt_state)\n",
    "    dparam = (jnp.sum((paramsf[0]-params[0])**2) + jnp.sum((paramsf[1]-params[1])**2)) / ( jnp.sum( params[0]**2 ) + jnp.sum(params[1]**2) )\n",
    "    param_movement += [  dparam ]\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.figure()\n",
    "for i,alpha in enumerate(alphas[:-1]):\n",
    "    print(alpha)\n",
    "    plt.plot(jnp.linspace(1,len(all_tr_losses[i]),len(all_tr_losses[i])), jnp.array(all_tr_losses[i]) / all_tr_losses[i][0], '--',  color = f'C{i}')\n",
    "    plt.plot(jnp.linspace(1,len(all_tr_losses[i]),len(all_tr_losses[i])), jnp.array(all_te_losses[i]) / all_te_losses[i][0],  color = f'C{i}', label = r'$\\alpha = 2^{%0.0f}$' % jnp.log2(alpha))\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'$t$',fontsize = 20)\n",
    "plt.ylabel('Loss',fontsize = 20)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c6f20-6dc0-48b0-9e77-fb052ef321f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Weight Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84395a76-cc41-47cb-a9e9-ab668ac15786",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_norms = [0.125,0.25,0.5,1.0,2.0]\n",
    "alpha = 1.0\n",
    "\n",
    "eta = 0.5 * N\n",
    "lamb = 0.0\n",
    "\n",
    "all_tr_losses_w = []\n",
    "all_te_losses_w = []\n",
    "all_acc_tr_w = []\n",
    "all_acc_te_w = []\n",
    "\n",
    "param_movement_w = []\n",
    "\n",
    "for i, wscale in enumerate(weight_norms):\n",
    "    a = wscale * random.normal(random.PRNGKey(0), (N, ))\n",
    "    W = wscale * random.normal(random.PRNGKey(0), (N, D))\n",
    "    params = [a, W]\n",
    "\n",
    "    opt_init, opt_update, get_params = optimizers.sgd( eta / wscale**2 )\n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "\n",
    "    loss_fn = jit(lambda p, X, y: jnp.mean( ( NN_func2(p, X,alpha)- NN_func2(params,X,alpha) - y )**2 / alpha**2 ))\n",
    "    acc_fn = jit(lambda p, X, y: jnp.mean( ( y * ( NN_func2(p, X,alpha)- NN_func2(params,X,alpha)) ) > 0.0 ))\n",
    "    reg_loss = jit(lambda p, X, y: loss_fn(p,X,y) + lamb / alpha * optimizers.l2_norm(p)**2 )\n",
    "\n",
    "    grad_loss = jit(grad(reg_loss,0))\n",
    "\n",
    "    tr_losses = []\n",
    "    te_losses = []\n",
    "    tr_acc = []\n",
    "    te_acc = []\n",
    "    for t in range(50000):\n",
    "        opt_state = opt_update(t, grad_loss(get_params(opt_state), X, y), opt_state)\n",
    "\n",
    "        if t % 2 == 0:\n",
    "            train_loss = alpha**2*loss_fn(get_params(opt_state), X, y)\n",
    "            test_loss = alpha**2*loss_fn(get_params(opt_state), Xt, yt)\n",
    "            tr_losses += [train_loss]\n",
    "            te_losses += [test_loss]\n",
    "            tr_acc += [ acc_fn(get_params(opt_state), X, y) ]\n",
    "            te_acc += [ acc_fn(get_params(opt_state), Xt, yt) ]\n",
    "            sys.stdout.write(f'\\r t: {t} | train loss: {train_loss} | test loss: {test_loss}')\n",
    "        if t % 10000 == 0:\n",
    "            print(\" \")\n",
    "\n",
    "    all_tr_losses_w += [tr_losses]\n",
    "    all_te_losses_w += [te_losses]\n",
    "    all_acc_tr_w += [tr_acc]\n",
    "    all_acc_te_w += [te_acc]\n",
    "\n",
    "    paramsf = get_params(opt_state)\n",
    "    dparam = (jnp.sum((paramsf[0]-params[0])**2) + jnp.sum((paramsf[1]-params[1])**2)) / ( jnp.sum(params[0]**2) + jnp.sum(params[1]**2) )\n",
    "    param_movement_w += [  dparam ]\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.figure()\n",
    "for i, wscale in enumerate(weight_norms):\n",
    "    print(alpha)\n",
    "    plt.plot(\n",
    "        jnp.linspace(1,len(all_tr_losses_w[i]),len(all_tr_losses_w[i])), \n",
    "        jnp.array(all_tr_losses_w[i]) / all_tr_losses_w[i][0], \n",
    "        '--',  \n",
    "        color = f'C{i}'\n",
    "    )\n",
    "    plt.plot(\n",
    "        jnp.linspace(1,len(all_tr_losses_w[i]),len(all_tr_losses_w[i])), \n",
    "        jnp.array(all_te_losses_w[i]) / all_te_losses_w[i][0],  \n",
    "        color = f'C{i}', \n",
    "        label = r'$\\sigma = 2^{%0.0f}$' % jnp.log2(wscale)\n",
    "    )\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('t',fontsize = 20)\n",
    "plt.ylabel('Loss',fontsize = 20)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f1aff-3c95-41c2-b397-cd987e9b6f2f",
   "metadata": {},
   "source": [
    "# Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80637675-0de3-4705-bdec-1dc02c98437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "  t: 2300 | train loss: 0.03071829117834568 | test loss: 0.38010162115097046 |frob ||kt-k0||_F: 0.065186478197574625"
     ]
    }
   ],
   "source": [
    "D = 100\n",
    "P = 550\n",
    "N = 500\n",
    "ntk_interval = 100 \n",
    "\n",
    "X = random.normal(random.PRNGKey(0), (D,P))\n",
    "Xt = random.normal(random.PRNGKey(1), (D,1000))\n",
    "beta = random.normal(random.PRNGKey(2), (D,))\n",
    "\n",
    "y = target_fn(beta, X)\n",
    "yt = target_fn(beta, Xt)\n",
    "\n",
    "a = random.normal(random.PRNGKey(0), (N, ))\n",
    "W = random.normal(random.PRNGKey(0), (N, D))\n",
    "params = [a, W]\n",
    "\n",
    "alpha=1.0\n",
    "eta = 0.5 * N\n",
    "lamb = 0.0\n",
    "opt_init, opt_update, get_params = optimizers.sgd(eta)\n",
    "\n",
    "epsilons = [2**(-2)]#,2**(-1),1,2,4]\n",
    "\n",
    "all_tr_losses_eps = []\n",
    "all_te_losses_eps = []\n",
    "all_acc_tr_eps = []\n",
    "all_acc_te_eps = []\n",
    "\n",
    "param_movement = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    def nn_wrapper(params, X):\n",
    "        return NN_func2(params, X.T, alpha=alpha, eps=eps)\n",
    "    \n",
    "    ntk_fn = nt.empirical_ntk_fn(nn_wrapper, vmap_axes=0, trace_axes=())\n",
    "    K_0 = ntk_fn(Xt.T, None, params)\n",
    "    np.save(f\"kernels_eps_sweep/k_0\", K_0)\n",
    "\n",
    "    opt_state = opt_init(params)\n",
    "    loss_fn = jit(lambda p, X, y: jnp.mean( ( NN_func2(p, X, alpha=alpha, eps=eps) - y )**2 / alpha**2 ))\n",
    "    acc_fn = jit(lambda p, X, y: jnp.mean( ( y * NN_func2(p, X, alpha=alpha, eps=eps)) > 0.0 ))\n",
    "    reg_loss = jit(lambda p, X, y: loss_fn(p,X,y) + lamb / alpha * optimizers.l2_norm(p)**2 )\n",
    "\n",
    "    grad_loss = jit(grad(reg_loss,0))\n",
    "\n",
    "    tr_losses = []\n",
    "    te_losses = []\n",
    "    tr_acc = []\n",
    "    te_acc = []\n",
    "    for t in range(60000):\n",
    "        opt_state = opt_update(t, grad_loss(get_params(opt_state), X, y), opt_state)\n",
    "        if t % 2 == 0:\n",
    "            train_loss = alpha**2*loss_fn(get_params(opt_state), X, y)\n",
    "            test_loss = alpha**2*loss_fn(get_params(opt_state), Xt, yt)\n",
    "            tr_losses += [train_loss]\n",
    "            te_losses += [test_loss]\n",
    "            tr_acc += [ acc_fn(get_params(opt_state), X, y) ]\n",
    "            te_acc += [ acc_fn(get_params(opt_state), Xt, yt) ]\n",
    "            # sys.stdout.write(f'\\r t: {t} | train loss: {train_loss} | test loss: {test_loss}')\n",
    "        if t % 10000 == 0:\n",
    "            print(\" \")\n",
    "\n",
    "        if t % ntk_interval == 0:\n",
    "            K_test = ntk_fn(Xt.T, None, get_params(opt_state))\n",
    "            sys.stdout.write(f'\\r  t: {t} | train loss: {train_loss} | test loss: {test_loss} |frob ||kt-k0||_F: {frobenius(K_test-K_0)}')\n",
    "            # np.save(f\"kernels_eps_sweep/k_{t}\", K_test) \n",
    "    all_tr_losses_eps += [tr_losses]\n",
    "    all_te_losses_eps += [te_losses]\n",
    "    all_acc_tr_eps += [tr_acc]\n",
    "    all_acc_te_eps += [te_acc]\n",
    "\n",
    "    paramsf = get_params(opt_state)\n",
    "    dparam = (jnp.sum((paramsf[0]-params[0])**2) + jnp.sum((paramsf[1]-params[1])**2)) / ( jnp.sum( params[0]**2 ) + jnp.sum(params[1]**2) )\n",
    "    param_movement += [dparam]\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.figure()\n",
    "for i,alpha in enumerate(alphas[:-1]):\n",
    "    print(alpha)\n",
    "    plt.plot(jnp.linspace(1,len(all_tr_losses[i]),len(all_tr_losses[i])), jnp.array(all_tr_losses[i]) / all_tr_losses[i][0], '--',  color = f'C{i}')\n",
    "    plt.plot(jnp.linspace(1,len(all_tr_losses[i]),len(all_tr_losses[i])), jnp.array(all_te_losses[i]) / all_te_losses[i][0],  color = f'C{i}', label = r'$\\alpha = 2^{%0.0f}$' % jnp.log2(alpha))\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'$t$',fontsize = 20)\n",
    "plt.ylabel('Loss',fontsize = 20)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1097040-0b3e-44cc-aa94-0a601b777abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
